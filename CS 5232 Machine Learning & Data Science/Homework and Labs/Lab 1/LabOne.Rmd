---
title: "Lab/HW 1"
author: "Noah Wong"
date: "February 5, 2020"
output:
  html_document:
    df_print: paged
  pdf_document: default
---


## Activity 1:
Do the following tasks:
  i. Download R
  ii. Download RStudio
  iii. Install packages: tidyverse, arules, caret, markdown
```{r, include=FALSE}
library(arules)
library(caret)
library(markdown)
library(tidyverse)
```

```{r,include=FALSE}
setwd("C:/Users/Work and School/Desktop/Data Mining")
```



## Activity 2: 
 Perform the following tasks
 
  a. Read the dataset contained in the file 'artificial_data.csv' into a data frame. 
```{r}
Artificial.df <- read.csv("artificial_data.csv")
```

  b. Plot a histogram of the attribute value. Do it with both base R graphics with ggplot.
```{r}
plot1 <- ggplot(Artificial.df, aes(x=value)) + 
          geom_histogram(bins = 40) 
plot2 <- hist(Artificial.df$value)
plot1
```

  c. Make a function named central which receives as input parameters: a vector v of real values, an integer num_rounds, an integer sample_ size and that does the following
  
    i. Draw num_rounds samples, each of size sample_size with replacement from the vector v. 
    
    ii. For each sample i obtained in part (i) compute its sample mean and its sample standard deviation.
    
    iii. Plot a histogram of the means of all the samples.
    
    iv. Plot a histogram of the standard deviation of all the samples. 
    
    v. The function must return a list containing just two real numbers: mean() and mean() over all the samples.
```{r}
central <- function(v, num_rounds, sample_size){
  Tempmatrix = matrix(0, num_rounds, sample_size)
  for(i in 1:num_rounds){
    Tempmatrix[i,] = sample(v,size = sample_size)
  }
  
  averages = matrix(0,num_rounds, 1)
  for(i in 1:num_rounds) {
    averages[i,] = sum(Tempmatrix[i,])/sample_size
  }
  
  sdeviations = matrix(0,num_rounds, 1)
  tempsum = 0
  for(i in 1:num_rounds) {
    tempsum = 0
    for(j in 1:sample_size){
      tempsum = tempsum + ((Tempmatrix[i,j] - averages[i,1])^2/(num_rounds-1))
    }
    sdeviations[i,] = sqrt(tempsum)
  }
  plot1 <- ggplot(data.frame(averages), aes(x=averages)) + 
           geom_histogram(bins = 10) + 
            ylab("Sample Mean")
  
  print(plot1)
  
  plot2<- ggplot(data.frame(sdeviations), aes(x=sdeviations)) + 
           geom_histogram(bins = 10) +
            xlab("Sample Standard Deviations")
  
  print(plot2)
  
  average.and.sdeviation = matrix(0,2,1)
  average.and.sdeviation[1,] = mean(averages)
  average.and.sdeviation[2,] = mean(sdeviations)
  
  return(average.and.sdeviation)
}
central(Artificial.df$value,100,10)

```
  
  


## Activity 4:
Fitting Linear Regression Models
  a. Find the optimum value for x using the normal equations directly
```{r}
z = c(3,7,11,18)
H =matrix(c(1,1,1,1,4,6,9,20,8,18,40.5,200),4,3)
ExactAns = solve(t(H)%*% H) %*% (t(H) %*% z) 
H
```
  
  b. Find the optimum value for x using the lm command in R
  
```{r}
df <- data.frame(H)
df$z = z
NoahsFirstModel <- lm(z~X2+X3,data = df)
summary(NoahsFirstModel)
```

## Activity 5: 
Gradient Descent Basics
  See the appendix with Activity 5



## Activity 6:
Gradient Descent Implementation
  a. Implement the gradient descent algorithm in R from scratch. 
```{r}
GradientDescent = function(x0, H, z, lambda, epsilon, max_iters) {
  
  error = 1
  n=0
  x = x0
  errors = matrix(0, max_iters,1)
  while(error > epsilon & n < max_iters ){
    n = n+1
    gradient = t(H)%*%H %*% x - t(H) %*% z
    
    previous = x
    x = x - lambda*gradient
    
    error = norm(previous - x, "2")
    errors[n,] = error
  }
 
  return(c(x,errors))
}

```
  b. Run your algorithm on the H and z matrices that we used in class to find x. Choose two different matrices for the initial guess and 3 different values for lamdba, then run your algorithm for all 6 combinations. 
```{r}
initial.guess1 = c(-4,3,1)
initial.guess2 = c(-6, 2,0)
lambda1 = 0.00001
lambda2 = 0.000005
lambda3 = 0.000001
epsilon = 0.0000001
max_iters = 10000
Guess11 = GradientDescent(initial.guess1, H, z, lambda1, epsilon, max_iters)
Guess12 = GradientDescent(initial.guess1, H, z, lambda2, epsilon, max_iters)
Guess13 = GradientDescent(initial.guess1, H, z, lambda3, epsilon, max_iters)
Guess21 = GradientDescent(initial.guess2, H, z, lambda1, epsilon, max_iters)
Guess22 = GradientDescent(initial.guess2, H, z, lambda2, epsilon, max_iters)
Guess23 = GradientDescent(initial.guess2, H, z, lambda3, epsilon, max_iters)
```


  c. Make a plot of the log of the difference between concurrent steps as a function of the iteration number i for each combination of values for initial guess and lamdba. 
```{r}
Errors11 = Guess11[4:(max_iters+ 3)]
Errors12 = Guess12[4:(max_iters+3)]
Errors13 = Guess13[4:(max_iters+3)]
Errors21 = Guess21[4:(max_iters+3)]
Errors22 = Guess22[4:(max_iters+3)]
Errors23 = Guess23[4:(max_iters+3)]
```

Creating the Plots
```{r}
Errors11 = as.data.frame(Errors11)
Errors11$index <- as.numeric(row.names(Errors11))
Errors11$logerror <- log10(Errors11$Errors11)
Plot11 <-  ggplot(data = Errors11,aes(x=index,y=logerror)) +geom_point() +ggtitle("Log Error for lamdba = 0.00001 and initial guess (-4,3,1)")

Errors12 = as.data.frame(Errors12)
Errors12$index <- as.numeric(row.names(Errors12))
Errors12$logerror <- log10(Errors12$Errors12)
Plot12 <-  ggplot(data = Errors12,aes(x=index,y=logerror)) +geom_point() +ggtitle("Log Error for lamdba = 0.000005 and initial guess (-4,3,1)")

Errors13 = as.data.frame(Errors13)
Errors13$index <- as.numeric(row.names(Errors13))
Errors13$logerror <- log10(Errors13$Errors13)
Plot13 <-  ggplot(data = Errors13,aes(x=index,y=logerror)) +geom_point() +ggtitle("Log Error for lamdba = 0.000001 and initial guess (-4,3,1)")

Errors21 = as.data.frame(Errors21)
Errors21$index <- as.numeric(row.names(Errors21))
Errors21$logerror <- log10(Errors21$Errors21)
Plot21 <-  ggplot(data = Errors21,aes(x=index,y=logerror)) +geom_point() +ggtitle("Log Error for lamdba = 0.00001 and initial guess (-6, 2,0)")

Errors22 = as.data.frame(Errors22)
Errors22$index <- as.numeric(row.names(Errors22))
Errors22$logerror <- log10(Errors22$Errors22)
Plot22 <-  ggplot(data = Errors22,aes(x=index,y=logerror)) +geom_point() +ggtitle("Log Error for lamdba = 0.000005 and initial guess (-6, 2,0)")

Errors23 = as.data.frame(Errors23)
Errors23$index <- as.numeric(row.names(Errors23))
Errors23$logerror <- log10(Errors23$Errors23)
Plot23 <-  ggplot(data = Errors23,aes(x=index,y=logerror)) +geom_point() +ggtitle("Log Error for lamdba = 0.000001 and initial guess (-6, 2,0)")
```

Here are my beautiful plots!
```{r}
Plot11
Plot12
Plot13
Plot21
Plot22
Plot23

```


  
  d. Comment on what you observe in your plots
  

There appears to be many patterns between the graphs. All of the graphes are decreasing so the log of the error decreases constantly. Also the error gets close to the correct answer very quickly and then after a number of iterations begins to slow considerable. I've heard of these graphes being called elbow graphs because they sort of look like an arm with an elbow at the point where the error begins to slow. Also the value with the higher lambda values got closer the error terms got together. Another thing to note is six different lambdas and initial guesses all went to the maximum number of iterations.

  
  
  

    
    
